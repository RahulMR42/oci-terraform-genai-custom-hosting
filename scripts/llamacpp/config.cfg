llamacpp_download_llm=YES # SET NO to avoid downloading llm from HF
llamacpp_model_hf_path="microsoft/Phi-3-mini-4k-instruct-gguf" #HF Path to download LLM
llamacpp_absolute_gguf="Phi-3-mini-4k-instruct-fp16.gguf" #Exact GGUF name (with in the LLM) to load.
llamacpp_mode_alias="msphi" #A short name for llm
llamacpp_llm_path="../llm_store" #LLM download path (This is set by base script),if you wish to change you need to create the folder mannualy and provide the path here
llamacpp_service_name=svc_llamacpp #Service name for llamacpp - alert if there is a name conflict with existing service
llamacpp_service_log_path="../../logs/llamacpp_service.log"
llamacpp_default_port=8000 #If you are using a different port make sure to enable security ingress rule for the port under your VCN>Subnet
llamacpp_openapi_port=8001 #If you are using a different port make sure to enable security ingress rule for the port under your VCN>Subnet
llamacpp_python_version=python3.11 #Python Version - Ensure it should be the same as that of when setting the alternates for python
llamacpp_hf_token=xxxxx #HF Token (ReadOnly access only required)
